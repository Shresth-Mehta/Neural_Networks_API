{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi4wj0KcYXahZZzTeiHTYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shresth-Mehta/Neural_Networks_API/blob/master/Self_Implemented_Neural_Network_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wt0-DOQx7VmA",
        "outputId": "c41c0c15-dd2f-4585-f872-862902f56807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 3b198426fb8c2113ad027f82a1cc9366f0b81b99"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.6/dist-packages (0.8.28)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (4.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.2)\n",
            "Requirement already satisfied, skipping upgrade: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.2)\n",
            "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.1)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEviuA4w2Nz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "#import wandb\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cRlHD4dK4b",
        "colab_type": "code",
        "outputId": "7034444f-ea50-4410-8227-869566bbe41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEwObToLaI2Q",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg-NGa-lDK2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(filename,model_obj):\n",
        "  mod=model_obj\n",
        "  if(filename==None):\n",
        "    filename=mod.model_run_name\n",
        "  fileobj=open(filename,'wb')\n",
        "  pickle.dump(mod,fileobj)\n",
        "    \n",
        "def restore_model(filename):                                                \n",
        "  return pickle.load(open(filename,'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUueBdvryNMM",
        "colab_type": "text"
      },
      "source": [
        "# **Model Class:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-UjRW_pyMHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self,in_dim,out_dim,loss,optimizer,learning_rate,regularization,moment,plot_loss=True,use_wandb=False,save_best=False,use_acc=False):\n",
        "    self.in_dim=in_dim\n",
        "    self.layers=[]\n",
        "    self.curr_dim=in_dim\n",
        "    self.out_dim=out_dim\n",
        "    self.optimizer=optimizer\n",
        "    self.regularization=regularization\n",
        "    self.loss_fn=loss()\n",
        "    self.loss_name=self.loss_fn.name()\n",
        "    self.learning_rate=learning_rate\n",
        "    self.moment=moment\n",
        "    self.plot_loss=plot_loss\n",
        "    self.use_wandb=use_wandb\n",
        "    self.save_best=save_best\n",
        "    self.acc=use_acc\n",
        "    self.model_run_name=\"no_name\"\n",
        "\n",
        "  def add_Dense(self,neuron_type,num_neurons):\n",
        "    layer=neuron_type(self.curr_dim,num_neurons)\n",
        "    self.layers.append(layer)\n",
        "    self.curr_dim=num_neurons\n",
        "      \n",
        "  def predict(self,X):                                                          # X should be an n-d array with (len,1)\n",
        "    y=X\n",
        "    for layer in self.layers:\n",
        "      y=layer.H_matrix(y)\n",
        "    return y\n",
        "\n",
        "  def __wandb_save(self,epoch,batch_size,learning_rate,momentum,loss):\n",
        "    wandb.init(project=\"ell_409_assignment_1\")\n",
        "    layers_list=[]\n",
        "    for layer in self.layers:\n",
        "      layers_list.append(str(layer.neuron_type)+\" \"+str(layer.num_neurons))\n",
        "    self.model_run_name=str(wandb.run.name)\n",
        "    print(self.model_run_name)\n",
        "    wandb.config.layers=layers_list\n",
        "    wandb.config.epoch=epoch\n",
        "    wandb.config.batch_size=batch_size\n",
        "    wandb.config.learning_rate=learning_rate\n",
        "    wandb.config.momentum=momentum\n",
        "    wandb.config.loss_function=loss.name()\n",
        "    wandb.config.accelerator=self.acc\n",
        "\n",
        "  def fit(self,x_train,y_train,x_test,y_test,n_epoch,batch_size):                            # plot layers is a list of number of layers that you wanna plot \n",
        "    save_model(\"initialization\",self)\n",
        "    dataset=np.array(list(zip(x_train,y_train)))\n",
        "    test_dataset=np.array(list(zip(x_test,y_test)))\n",
        "    rec_against=[]\n",
        "    train_error_mse_list=[]\n",
        "    test_error_mse_list=[]\n",
        "    train_error_ce_list=[]\n",
        "    test_error_ce_list=[]\n",
        "    if(self.use_wandb==True):\n",
        "      self.__wandb_save(n_epoch,batch_size,self.learning_rate,self.moment,self.loss_fn)\n",
        "    train_error_ce,train_error_mse,train_accuracy,unused=self.__get_logs(dataset)\n",
        "    test_error_ce,test_error_mse,test_accuracy,num_teg=self.__get_logs(test_dataset)\n",
        "    print(\"epoch:\",\"0\",\"train_error_ce:\",train_error_ce,\"train_error_mse:\",train_error_mse,\"train_accuracy:\",train_accuracy,\"num_eg:\",unused)\n",
        "    print(\"test_error_ce:\",test_error_ce,\"test_error_mse:\",test_error_mse,\"test_accuracy:\",test_accuracy,\"num_eg:\",num_teg)\n",
        "    min_testing_error=test_error_ce\n",
        "    for epoch in range(n_epoch):\n",
        "      num_eg=0\n",
        "      batch_itr=1\n",
        "      rec_against.append(epoch+1)\n",
        "      for data_point,label in dataset:\n",
        "        num_eg+=1\n",
        "        data_point=data_point.reshape(len(data_point),1)\n",
        "        label=label.reshape(len(label),1)                                       # data_point and label have to be numpy arrays (n,1)\n",
        "        #pred=self.predict(data_point)\n",
        "        if(batch_size==None):\n",
        "          self.__back_propagate(data_point,label)\n",
        "        else:\n",
        "          if(self.acc==True and batch_itr==1):\n",
        "            self.__accelerate()\n",
        "          self.__batch_propagate(data_point,label)\n",
        "          if(batch_itr==batch_size or num_eg==len(x_train)):\n",
        "            self.__batch_update(batch_size)\n",
        "            batch_itr=0\n",
        "          batch_itr+=1\n",
        "      np.random.shuffle(dataset)\n",
        "      train_error_ce,train_error_mse,train_accuracy,unused=self.__get_logs(dataset)\n",
        "      test_error_ce,test_error_mse,test_accuracy,num_teg=self.__get_logs(test_dataset)\n",
        "      print(\"epoch:\",epoch+1,\"train_error_ce:\",train_error_ce,\"train_error_mse:\",train_error_mse,\"train_accuracy:\",train_accuracy,\"num_eg:\",num_eg)\n",
        "      print(\"test_error_ce:\",test_error_ce,\"test_error_mse:\",test_error_mse,\"test_accuracy:\",test_accuracy,\"num_eg:\",num_teg) \n",
        "      if(self.use_wandb==True):\n",
        "        wandb.log({\"train_loss_mse\":train_error_mse,\"train_loss_ce\":train_error_ce,\"validation_loss_mse\":test_error_mse,\"validation_loss_ce\":test_error_ce,\"train_accuracy\":train_accuracy,\"validation_accuracy\":test_accuracy})\n",
        "      if(self.plot_loss==True):\n",
        "        train_error_mse_list.append(train_error_mse)\n",
        "        test_error_mse_list.append(test_error_mse)\n",
        "        train_error_ce_list.append(train_error_ce)\n",
        "        test_error_ce_list.append(test_error_ce)\n",
        "      if(min_testing_error>=test_error_ce and test_accuracy>96.0 and self.save_best==True):\n",
        "        min_testing_error=test_error_ce\n",
        "        self.__save_model(\"best_model\")\n",
        "\n",
        "    if(self.plot_loss==True):\n",
        "      plt.subplot(1,2,1)\n",
        "      plt.plot(rec_against,train_error_mse_list,label=\"train_error_mse\")\n",
        "      plt.plot(rec_against,test_error_mse_list,label=\"test_error_mse\")\n",
        "      plt.legend()\n",
        "      plt.subplot(1,2,2)\n",
        "      plt.plot(rec_against,train_error_ce_list,label=\"train_error_ce\")\n",
        "      plt.plot(rec_against,test_error_ce_list,label=\"test_error_ce\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  def __get_logs(self,dataset):\n",
        "    c_e_loss=0\n",
        "    m_s_e_loss=0\n",
        "    correct=0\n",
        "    loss_c_e=Cross_entropy_loss()\n",
        "    loss_m_s_e=Squared_error_loss()\n",
        "    for data_point,label in dataset:\n",
        "      data_point=data_point.reshape(len(data_point),1)\n",
        "      label=label.reshape(len(label),1)\n",
        "      pred=self.predict(data_point)\n",
        "      #sum_error+=self.loss_fn.expression(pred,label)\n",
        "      c_e_loss+=loss_c_e.expression(pred,label)\n",
        "      m_s_e_loss+=loss_m_s_e.expression(pred,label)\n",
        "      if(np.argmax(pred)==np.argmax(label)):\n",
        "        correct+=1\n",
        "    #sum_error=sum_error/len(dataset)\n",
        "    c_e_loss=c_e_loss/len(dataset)\n",
        "    m_s_e_loss=m_s_e_loss/len(dataset)\n",
        "    accuracy=correct*100.00/len(dataset) \n",
        "    return c_e_loss,m_s_e_loss,accuracy,len(dataset)\n",
        "\n",
        "  def validate(self,x,y):\n",
        "    dataset=np.array(list(zip(x,y)))\n",
        "    ce_error,mse_error,acc,num=self.__get_logs(dataset)\n",
        "    print(\"error_ce:\",ce_error,\"error_mse:\",mse_error,\"accuracy:\",acc)\n",
        "\n",
        "\n",
        "  def __accelerate(self):\n",
        "    #print(\"acc_called\")\n",
        "    for layer in self.layers:\n",
        "      layer.W=layer.W-self.moment*layer.M_W\n",
        "      layer.B=layer.B-self.moment*layer.M_B\n",
        "\n",
        "  def __back_propagate(self,x,y):\n",
        "    if(self.acc==True):\n",
        "      self.__accelerate()\n",
        "    pred=self.predict(x)\n",
        "    D_A=self.loss_fn.Der(self.layers[-1],y)\n",
        "    for l_num in range(len(self.layers)-1,0,-1):\n",
        "      D_W=np.matmul(D_A,self.layers[l_num-1].H.transpose())\n",
        "      D_B=D_A\n",
        "      D_H=np.matmul(self.layers[l_num].W.transpose(),D_A)\n",
        "      D_A=(D_H*self.layers[l_num-1].Der_a())\n",
        "      self.__update_weights(self.layers[l_num],D_W,D_B)  \n",
        "    D_W=np.matmul(D_A,x.transpose())\n",
        "    self.__update_weights(self.layers[0],D_W,D_A)\n",
        "  \n",
        "  def __batch_update(self,batch_size):\n",
        "    for layer in self.layers:\n",
        "      DW=layer.DW/batch_size\n",
        "      DB=layer.DB/batch_size\n",
        "      self.__update_weights(layer,DW,DB)\n",
        "      layer.DW=np.zeros((layer.num_neurons,layer.in_dim))\n",
        "      layer.DB=np.zeros((layer.num_neurons,1))\n",
        "\n",
        "  def __batch_propagate(self,x,y):\n",
        "    pred=self.predict(x)\n",
        "    D_A=self.loss_fn.Der(self.layers[-1],y)\n",
        "    for l_num in range(len(self.layers)-1,0,-1):\n",
        "      D_W=np.matmul(D_A,self.layers[l_num-1].H.transpose())\n",
        "      D_B=D_A\n",
        "      self.layers[l_num].DW+=D_W\n",
        "      self.layers[l_num].DB+=D_B\n",
        "      D_H=np.matmul(self.layers[l_num].W.transpose(),D_A)\n",
        "      D_A=(D_H*self.layers[l_num-1].Der_a()) \n",
        "    D_W=np.matmul(D_A,x.transpose())\n",
        "    self.layers[0].DW+=D_W\n",
        "    self.layers[0].DB+=D_A\n",
        "  \n",
        "  def __update_weights(self,layer,D_W,D_B):\n",
        "    if(self.acc==True):\n",
        "      layer.W=layer.W-self.learning_rate*D_W\n",
        "      layer.B=layer.B-self.learning_rate*D_B\n",
        "      layer.M_W=self.moment*layer.M_W+self.learning_rate*D_W\n",
        "      layer.B_W=self.moment*layer.M_B+self.learning_rate*D_B   \n",
        "    else:\n",
        "      layer.M_W=self.learning_rate*D_W+self.moment*layer.M_W\n",
        "      layer.M_B=self.learning_rate*D_B+self.moment*layer.M_B\n",
        "      layer.W=layer.W-layer.M_W\n",
        "      layer.B=layer.B-layer.M_B\n",
        "      \n",
        "  \n",
        "  def __save_model(self,filename):\n",
        "    mod=self\n",
        "    fileobj=open(filename,'wb')\n",
        "    pickle.dump(mod,fileobj)\n",
        "  \n",
        "  def save_in_wandb(self):\n",
        "    self.__save_model(os.path.join(wandb.run.dir,\"model\"))\n",
        "\n",
        "  def confusion_matrix(self,X,Y):\n",
        "    y_pred=[]\n",
        "    y_true=[]\n",
        "    dataset=np.array(list(zip(X,Y)))\n",
        "    for data_point,label in dataset:\n",
        "      data_point=data_point.reshape(len(data_point),1)\n",
        "      label=label.reshape(len(label),1)\n",
        "      pred=self.predict(data_point)\n",
        "      y_pred.append(np.argmax(pred))\n",
        "      y_true.append(np.argmax(label))\n",
        "    matrix=np.zeros((self.out_dim,self.out_dim))\n",
        "    for i in range(len(y_true)):\n",
        "      matrix[y_true[i]][y_pred[i]] +=1\n",
        "    return (matrix,y_pred,y_true)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_DtT8g0ZlL7",
        "colab_type": "text"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gazZILuSylLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseLayer:\n",
        "  def __init__(self,in_dim,num_neurons):\n",
        "    self.W=np.random.randn(num_neurons,in_dim)*np.sqrt(2/(num_neurons+in_dim))\n",
        "    self.B=np.zeros((num_neurons,1))                                            # Bias is not uniform for a layer\n",
        "    self.num_neurons=num_neurons\n",
        "    self.in_dim=in_dim\n",
        "    self.A=np.zeros((num_neurons,1))\n",
        "    self.H=np.zeros((num_neurons,1))\n",
        "    self.rec_weights=[np.sum(np.absolute(self.W))/np.size(self.W)]\n",
        "    self.neuron_type=None\n",
        "    self.DW=np.zeros((num_neurons,in_dim))\n",
        "    self.DB=np.zeros((num_neurons,1))\n",
        "    self.M_W=np.zeros((num_neurons,in_dim))\n",
        "    self.M_B=np.zeros((num_neurons,1))\n",
        "\n",
        "  def update_weights_rec(self):\n",
        "    self.rec_weights.append(np.sum(np.absolute(self.W))/np.size(self.W))\n",
        "\n",
        "  def A_matrix(self,X):\n",
        "    self.A=np.matmul(self.W,X)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc9_NZu8TG9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid_Layer(DenseLayer):\n",
        "  def __init__(self,in_dim,num_neurons):\n",
        "    DenseLayer.__init__(self,in_dim,num_neurons)\n",
        "    self.neuron_type=\"sigmoid\"\n",
        "  def h(self,x):\n",
        "    return 1.0/(1.0+np.exp(0-x))\n",
        "  def Der_a(self):\n",
        "    return self.H*(1-self.H)\n",
        "  def H_matrix(self,X):\n",
        "    self.A_matrix(X)\n",
        "    self.H=self.h(self.A+self.B)\n",
        "    return self.H\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXxAlpvOqBb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax_Layer(DenseLayer):\n",
        "  def __init__(self,in_dim,num_neurons):\n",
        "    DenseLayer.__init__(self,in_dim,num_neurons)\n",
        "    self.neuron_type=\"softmax\"\n",
        "  def h(self,x):\n",
        "    e=np.exp(x)\n",
        "    ans=np.sum(e)\n",
        "    return np.divide(e,ans)\n",
        "  def Der_a(self):\n",
        "    return self.H*(1-self.H)                                                                     # write the derivative(not needed)\n",
        "  def H_matrix(self,X):\n",
        "    self.A_matrix(X)\n",
        "    self.H=self.h(self.A+self.B)\n",
        "    return self.H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vER8-KS4ZvzC",
        "colab_type": "text"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_0suW2ECIuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cross_entropy_loss:\n",
        "  def name(self):\n",
        "    return \"Cross_entropy_loss\"\n",
        "  def expression(self,y_hat,y):\n",
        "    return 0-np.log(y_hat[np.argmax(y)])\n",
        "  def Der(self,layer,y):\n",
        "    if(layer.neuron_type==\"softmax\"):\n",
        "      return layer.H-y\n",
        "    elif(layer.neuron_type==\"sigmoid\"):\n",
        "      return (layer.H-1)*y                                                                   # complete the return statement\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIjlLIBwX4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Squared_error_loss:\n",
        "  def name(self):\n",
        "    return \"Squared_error_loss\"\n",
        "  def expression(self,y_hat,y):\n",
        "    return np.sum((y-y_hat)**2)/(2*len(y))\n",
        "  def Der(self,layer,y):\n",
        "    if(layer.neuron_type==\"softmax\"):\n",
        "      return -(y-layer.H)*layer.H*(1-layer.H)/len(y)\n",
        "    elif(layer.neuron_type==\"sigmoid\"):\n",
        "      return -(y-layer.H)*layer.H*(1-layer.H)/len(y)\n",
        "                                                                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd3jb5HJ1Hej",
        "colab_type": "text"
      },
      "source": [
        "# Getting Mnist data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqlV1QPCzDYw",
        "colab_type": "code",
        "outputId": "0bfab1ae-75a1-4984-9c3d-bf8968765a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mnist_dataframe=pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\",\n",
        "sep=\",\",header=None)\n",
        "mnist_dataframe=mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "y=np.array(mnist_dataframe[0])\n",
        "l=len(y)\n",
        "y_labels=np.zeros((l,10))\n",
        "for i,j in zip(y_labels,y):\n",
        "  i[j]=1\n",
        "mnist_dataframe=mnist_dataframe.drop(columns=[0])\n",
        "x=mnist_dataframe.to_numpy()\n",
        "x=x/255.0\n",
        "print(x.shape,y_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 784) (20000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNNCeVM7CDTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_train_split(frac,x,y):\n",
        "  num_train=int((1-frac)*len(x))\n",
        "  x_train=x[:num_train][:]\n",
        "  y_train=y[:num_train][:]\n",
        "  x_test=x[num_train:][:]\n",
        "  y_test=y[num_train:][:]\n",
        "  return (x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPZOmXqEEzZt",
        "colab_type": "code",
        "outputId": "d0099108-99cf-4f0f-cc76-e5c41039cb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train,y_train,x_test,y_test=test_train_split(0.05,x,y_labels)\n",
        "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19000, 784) (19000, 10) (1000, 784) (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Arbwmdx_qn",
        "colab_type": "code",
        "outputId": "f97b7d37-6809-4da0-c351-b0407802f661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_x,val_y,non1,non2=test_train_split(0,x,y_labels)\n",
        "print(val_x.shape,val_y.shape,non1.shape,non2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 784) (20000, 10) (0, 784) (0, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr_XzK9iZMqV",
        "colab_type": "text"
      },
      "source": [
        "# Getting Smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9v3DmXDZSYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path):\n",
        "  mnist_dataframe=pd.read_csv(path,\n",
        "  sep=\",\",header=None)\n",
        "  mnist_dataframe=mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "  y=np.array(mnist_dataframe[25])\n",
        "  l=len(y)\n",
        "  y_labels=np.zeros((l,10))\n",
        "  for i,j in zip(y_labels,y):\n",
        "    i[j]=1\n",
        "  mnist_dataframe=mnist_dataframe.drop(columns=[25])\n",
        "  x=mnist_dataframe.to_numpy()\n",
        "  #x=x/255.0\n",
        "  print(x.shape,y_labels.shape)\n",
        "  return x,y_labels\n",
        "\n",
        "def test_train_split(frac,x,y):\n",
        "  num_train=int((1-frac)*len(x))\n",
        "  x_train=x[:num_train][:]\n",
        "  y_train=y[:num_train][:]\n",
        "  x_test=x[num_train:][:]\n",
        "  y_test=y[num_train:][:]\n",
        "  return (x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roWmc51Wl6sa",
        "colab_type": "code",
        "outputId": "8cd348ef-807e-41ea-9692-3d6ce355bb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x,y=get_data(\"/content/drive/My Drive/trainData.csv\")\n",
        "x_train,y_train,x_test,y_test=test_train_split(0,x,y)\n",
        "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000, 25) (3000, 10)\n",
            "(3000, 25) (3000, 10) (0, 25) (0, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_cxoklrNiCQ",
        "colab_type": "text"
      },
      "source": [
        "# Getting The Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXb_tvoNm4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_data(path):\n",
        "  mnist_dataframe=pd.read_csv(path,\n",
        "  sep=\",\",header=None)\n",
        "  x=mnist_dataframe.to_numpy()\n",
        "  #x=x/255.0\n",
        "  print(x.shape)\n",
        "  return x\n",
        "\n",
        "def get_predictions(dataset,model_obj):\n",
        "  predictions=[]\n",
        "  id=[]\n",
        "  i=1\n",
        "  for data_point in dataset:\n",
        "    data_point=data_point.reshape(len(data_point),1)                                     # data_point and label have to be numpy arrays (n,1)\n",
        "    pred=model_obj.predict(data_point)\n",
        "    predictions.append(np.argmax(pred))\n",
        "    id.append(i)\n",
        "    i+=1\n",
        "  submission=pd.DataFrame()\n",
        "  submission['id']=id\n",
        "  submission['label']=predictions\n",
        "  return submission\n",
        "\n",
        "def get_submission_csv(test_data_path,model_obj,pred_csv_file_path):\n",
        "  test_data=get_test_data(test_data_path)\n",
        "  submission=get_predictions(test_data,model_obj)\n",
        "  submission.to_csv(pred_csv_file_path,index=False)\n",
        "  print(submission.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icsqeAJmYTJ_",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PFY2NqRyy5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_1=Model(25,10,Cross_entropy_loss,None,0.01,None,0.01,True,False,True,False)   #moment,plot,wandb,save_best,accelerator                                                # 10-15-10 gave nan on weights of layer 2 and 3\n",
        "nn_1.add_Dense(Sigmoid_Layer,80)\n",
        "nn_1.add_Dense(Sigmoid_Layer,60)\n",
        "nn_1.add_Dense(Softmax_Layer,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kdXjvS8y26R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_1.learning_rate=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P87QsIwHLAyN",
        "colab_type": "code",
        "outputId": "a30c4528-daad-46a5-b8ec-379057b9bf41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nn_1.fit(x_train,y_train,x_train,y_train,50,4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 train_error_ce: [1.20932357] train_error_mse: 0.02825950296120366 train_accuracy: 70.2 num_eg: 3000\n",
            "test_error_ce: [1.20932357] test_error_mse: 0.02825950296120366 test_accuracy: 70.2 num_eg: 3000\n",
            "epoch: 1 train_error_ce: [1.1846922] train_error_mse: 0.028192603743833904 train_accuracy: 54.7 num_eg: 3000\n",
            "test_error_ce: [1.1846922] test_error_mse: 0.028192603743833953 test_accuracy: 54.7 num_eg: 3000\n",
            "epoch: 2 train_error_ce: [1.00854944] train_error_mse: 0.024221373507230377 train_accuracy: 63.9 num_eg: 3000\n",
            "test_error_ce: [1.00854944] test_error_mse: 0.024221373507230463 test_accuracy: 63.9 num_eg: 3000\n",
            "epoch: 3 train_error_ce: [0.7496409] train_error_mse: 0.017708300986113038 train_accuracy: 75.2 num_eg: 3000\n",
            "test_error_ce: [0.7496409] test_error_mse: 0.017708300986113094 test_accuracy: 75.2 num_eg: 3000\n",
            "epoch: 4 train_error_ce: [0.70088833] train_error_mse: 0.016429431327596455 train_accuracy: 76.16666666666667 num_eg: 3000\n",
            "test_error_ce: [0.70088833] test_error_mse: 0.01642943132759642 test_accuracy: 76.16666666666667 num_eg: 3000\n",
            "epoch: 5 train_error_ce: [0.64009284] train_error_mse: 0.014887696050374804 train_accuracy: 79.8 num_eg: 3000\n",
            "test_error_ce: [0.64009284] test_error_mse: 0.014887696050374778 test_accuracy: 79.8 num_eg: 3000\n",
            "epoch: 6 train_error_ce: [0.54710259] train_error_mse: 0.012161555282962613 train_accuracy: 84.63333333333334 num_eg: 3000\n",
            "test_error_ce: [0.54710259] test_error_mse: 0.012161555282962627 test_accuracy: 84.63333333333334 num_eg: 3000\n",
            "epoch: 7 train_error_ce: [0.61926353] train_error_mse: 0.014677133494829483 train_accuracy: 79.43333333333334 num_eg: 3000\n",
            "test_error_ce: [0.61926353] test_error_mse: 0.014677133494829481 test_accuracy: 79.43333333333334 num_eg: 3000\n",
            "epoch: 8 train_error_ce: [0.53086851] train_error_mse: 0.01208707423706003 train_accuracy: 84.06666666666666 num_eg: 3000\n",
            "test_error_ce: [0.53086851] test_error_mse: 0.012087074237060028 test_accuracy: 84.06666666666666 num_eg: 3000\n",
            "epoch: 9 train_error_ce: [0.54347603] train_error_mse: 0.012399359640000853 train_accuracy: 83.03333333333333 num_eg: 3000\n",
            "test_error_ce: [0.54347603] test_error_mse: 0.012399359640000862 test_accuracy: 83.03333333333333 num_eg: 3000\n",
            "epoch: 10 train_error_ce: [0.53397556] train_error_mse: 0.012351569200185748 train_accuracy: 83.26666666666667 num_eg: 3000\n",
            "test_error_ce: [0.53397556] test_error_mse: 0.012351569200185734 test_accuracy: 83.26666666666667 num_eg: 3000\n",
            "epoch: 11 train_error_ce: [0.67390258] train_error_mse: 0.015808004092885658 train_accuracy: 77.66666666666667 num_eg: 3000\n",
            "test_error_ce: [0.67390258] test_error_mse: 0.01580800409288572 test_accuracy: 77.66666666666667 num_eg: 3000\n",
            "epoch: 12 train_error_ce: [0.52617336] train_error_mse: 0.01208158230425996 train_accuracy: 83.6 num_eg: 3000\n",
            "test_error_ce: [0.52617336] test_error_mse: 0.012081582304259958 test_accuracy: 83.6 num_eg: 3000\n",
            "epoch: 13 train_error_ce: [0.42504338] train_error_mse: 0.009612230517279151 train_accuracy: 87.23333333333333 num_eg: 3000\n",
            "test_error_ce: [0.42504338] test_error_mse: 0.00961223051727916 test_accuracy: 87.23333333333333 num_eg: 3000\n",
            "epoch: 14 train_error_ce: [0.43288168] train_error_mse: 0.009863031038281763 train_accuracy: 86.5 num_eg: 3000\n",
            "test_error_ce: [0.43288168] test_error_mse: 0.00986303103828176 test_accuracy: 86.5 num_eg: 3000\n",
            "epoch: 15 train_error_ce: [0.46300312] train_error_mse: 0.010592294176277562 train_accuracy: 85.5 num_eg: 3000\n",
            "test_error_ce: [0.46300312] test_error_mse: 0.010592294176277592 test_accuracy: 85.5 num_eg: 3000\n",
            "epoch: 16 train_error_ce: [0.45471024] train_error_mse: 0.010653024716498361 train_accuracy: 85.43333333333334 num_eg: 3000\n",
            "test_error_ce: [0.45471024] test_error_mse: 0.010653024716498384 test_accuracy: 85.43333333333334 num_eg: 3000\n",
            "epoch: 17 train_error_ce: [0.42751454] train_error_mse: 0.009563150338083243 train_accuracy: 86.83333333333333 num_eg: 3000\n",
            "test_error_ce: [0.42751454] test_error_mse: 0.009563150338083224 test_accuracy: 86.83333333333333 num_eg: 3000\n",
            "epoch: 18 train_error_ce: [0.53976959] train_error_mse: 0.01267388652118331 train_accuracy: 82.4 num_eg: 3000\n",
            "test_error_ce: [0.53976959] test_error_mse: 0.012673886521183345 test_accuracy: 82.4 num_eg: 3000\n",
            "epoch: 19 train_error_ce: [0.44544485] train_error_mse: 0.010390901777286698 train_accuracy: 85.26666666666667 num_eg: 3000\n",
            "test_error_ce: [0.44544485] test_error_mse: 0.010390901777286674 test_accuracy: 85.26666666666667 num_eg: 3000\n",
            "epoch: 20 train_error_ce: [0.45538745] train_error_mse: 0.010569047768413851 train_accuracy: 85.36666666666666 num_eg: 3000\n",
            "test_error_ce: [0.45538745] test_error_mse: 0.010569047768413858 test_accuracy: 85.36666666666666 num_eg: 3000\n",
            "epoch: 21 train_error_ce: [0.39590902] train_error_mse: 0.009052849516552776 train_accuracy: 87.5 num_eg: 3000\n",
            "test_error_ce: [0.39590902] test_error_mse: 0.009052849516552768 test_accuracy: 87.5 num_eg: 3000\n",
            "epoch: 22 train_error_ce: [0.49403795] train_error_mse: 0.011726609256437023 train_accuracy: 83.7 num_eg: 3000\n",
            "test_error_ce: [0.49403795] test_error_mse: 0.011726609256437034 test_accuracy: 83.7 num_eg: 3000\n",
            "epoch: 23 train_error_ce: [0.38948113] train_error_mse: 0.008917478695669957 train_accuracy: 88.2 num_eg: 3000\n",
            "test_error_ce: [0.38948113] test_error_mse: 0.008917478695669938 test_accuracy: 88.2 num_eg: 3000\n",
            "epoch: 24 train_error_ce: [0.37724926] train_error_mse: 0.008620262069007126 train_accuracy: 88.3 num_eg: 3000\n",
            "test_error_ce: [0.37724926] test_error_mse: 0.0086202620690071 test_accuracy: 88.3 num_eg: 3000\n",
            "epoch: 25 train_error_ce: [0.41914455] train_error_mse: 0.010120664118736326 train_accuracy: 86.43333333333334 num_eg: 3000\n",
            "test_error_ce: [0.41914455] test_error_mse: 0.01012066411873632 test_accuracy: 86.43333333333334 num_eg: 3000\n",
            "epoch: 26 train_error_ce: [0.44238883] train_error_mse: 0.010496018456218004 train_accuracy: 85.5 num_eg: 3000\n",
            "test_error_ce: [0.44238883] test_error_mse: 0.010496018456218012 test_accuracy: 85.5 num_eg: 3000\n",
            "epoch: 27 train_error_ce: [0.37864162] train_error_mse: 0.00860803363260037 train_accuracy: 87.93333333333334 num_eg: 3000\n",
            "test_error_ce: [0.37864162] test_error_mse: 0.008608033632600364 test_accuracy: 87.93333333333334 num_eg: 3000\n",
            "epoch: 28 train_error_ce: [0.40357765] train_error_mse: 0.009434651991102095 train_accuracy: 87.2 num_eg: 3000\n",
            "test_error_ce: [0.40357765] test_error_mse: 0.009434651991102072 test_accuracy: 87.2 num_eg: 3000\n",
            "epoch: 29 train_error_ce: [0.38758649] train_error_mse: 0.009027398548725281 train_accuracy: 87.66666666666667 num_eg: 3000\n",
            "test_error_ce: [0.38758649] test_error_mse: 0.009027398548725269 test_accuracy: 87.66666666666667 num_eg: 3000\n",
            "epoch: 30 train_error_ce: [0.38930801] train_error_mse: 0.009094608576328775 train_accuracy: 87.53333333333333 num_eg: 3000\n",
            "test_error_ce: [0.38930801] test_error_mse: 0.009094608576328777 test_accuracy: 87.53333333333333 num_eg: 3000\n",
            "epoch: 31 train_error_ce: [0.32639105] train_error_mse: 0.007560607738383641 train_accuracy: 89.66666666666667 num_eg: 3000\n",
            "test_error_ce: [0.32639105] test_error_mse: 0.007560607738383611 test_accuracy: 89.66666666666667 num_eg: 3000\n",
            "epoch: 32 train_error_ce: [0.38160036] train_error_mse: 0.009085727718610086 train_accuracy: 87.86666666666666 num_eg: 3000\n",
            "test_error_ce: [0.38160036] test_error_mse: 0.009085727718610062 test_accuracy: 87.86666666666666 num_eg: 3000\n",
            "epoch: 33 train_error_ce: [0.42313059] train_error_mse: 0.009925196501158378 train_accuracy: 86.3 num_eg: 3000\n",
            "test_error_ce: [0.42313059] test_error_mse: 0.009925196501158378 test_accuracy: 86.3 num_eg: 3000\n",
            "epoch: 34 train_error_ce: [0.34531277] train_error_mse: 0.008023140140861414 train_accuracy: 89.16666666666667 num_eg: 3000\n",
            "test_error_ce: [0.34531277] test_error_mse: 0.008023140140861416 test_accuracy: 89.16666666666667 num_eg: 3000\n",
            "epoch: 35 train_error_ce: [0.34157078] train_error_mse: 0.007962146283185938 train_accuracy: 88.96666666666667 num_eg: 3000\n",
            "test_error_ce: [0.34157078] test_error_mse: 0.007962146283185973 test_accuracy: 88.96666666666667 num_eg: 3000\n",
            "epoch: 36 train_error_ce: [0.32912952] train_error_mse: 0.00776515955405769 train_accuracy: 89.26666666666667 num_eg: 3000\n",
            "test_error_ce: [0.32912952] test_error_mse: 0.00776515955405769 test_accuracy: 89.26666666666667 num_eg: 3000\n",
            "epoch: 37 train_error_ce: [0.32891545] train_error_mse: 0.007686930640857241 train_accuracy: 89.43333333333334 num_eg: 3000\n",
            "test_error_ce: [0.32891545] test_error_mse: 0.007686930640857252 test_accuracy: 89.43333333333334 num_eg: 3000\n",
            "epoch: 38 train_error_ce: [0.3326303] train_error_mse: 0.007766146404272194 train_accuracy: 89.4 num_eg: 3000\n",
            "test_error_ce: [0.3326303] test_error_mse: 0.007766146404272195 test_accuracy: 89.4 num_eg: 3000\n",
            "epoch: 39 train_error_ce: [0.33787101] train_error_mse: 0.008138687485445332 train_accuracy: 88.9 num_eg: 3000\n",
            "test_error_ce: [0.33787101] test_error_mse: 0.008138687485445332 test_accuracy: 88.9 num_eg: 3000\n",
            "epoch: 40 train_error_ce: [0.32409788] train_error_mse: 0.007765082414053177 train_accuracy: 89.03333333333333 num_eg: 3000\n",
            "test_error_ce: [0.32409788] test_error_mse: 0.00776508241405317 test_accuracy: 89.03333333333333 num_eg: 3000\n",
            "epoch: 41 train_error_ce: [0.36360377] train_error_mse: 0.00886626379967581 train_accuracy: 87.7 num_eg: 3000\n",
            "test_error_ce: [0.36360377] test_error_mse: 0.008866263799675774 test_accuracy: 87.7 num_eg: 3000\n",
            "epoch: 42 train_error_ce: [0.30051763] train_error_mse: 0.0070908133825528215 train_accuracy: 90.43333333333334 num_eg: 3000\n",
            "test_error_ce: [0.30051763] test_error_mse: 0.00709081338255285 test_accuracy: 90.43333333333334 num_eg: 3000\n",
            "epoch: 43 train_error_ce: [0.39818492] train_error_mse: 0.009366563744308649 train_accuracy: 86.86666666666666 num_eg: 3000\n",
            "test_error_ce: [0.39818492] test_error_mse: 0.009366563744308633 test_accuracy: 86.86666666666666 num_eg: 3000\n",
            "epoch: 44 train_error_ce: [0.30916667] train_error_mse: 0.007344456254749234 train_accuracy: 89.86666666666666 num_eg: 3000\n",
            "test_error_ce: [0.30916667] test_error_mse: 0.007344456254749253 test_accuracy: 89.86666666666666 num_eg: 3000\n",
            "epoch: 45 train_error_ce: [0.33719712] train_error_mse: 0.008038141963797957 train_accuracy: 89.0 num_eg: 3000\n",
            "test_error_ce: [0.33719712] test_error_mse: 0.008038141963797932 test_accuracy: 89.0 num_eg: 3000\n",
            "epoch: 46 train_error_ce: [0.31617137] train_error_mse: 0.007525616284329692 train_accuracy: 89.73333333333333 num_eg: 3000\n",
            "test_error_ce: [0.31617137] test_error_mse: 0.007525616284329718 test_accuracy: 89.73333333333333 num_eg: 3000\n",
            "epoch: 47 train_error_ce: [0.34119564] train_error_mse: 0.008475369828184323 train_accuracy: 88.26666666666667 num_eg: 3000\n",
            "test_error_ce: [0.34119564] test_error_mse: 0.008475369828184304 test_accuracy: 88.26666666666667 num_eg: 3000\n",
            "epoch: 48 train_error_ce: [0.31432189] train_error_mse: 0.00736849802970316 train_accuracy: 89.76666666666667 num_eg: 3000\n",
            "test_error_ce: [0.31432189] test_error_mse: 0.007368498029703165 test_accuracy: 89.76666666666667 num_eg: 3000\n",
            "epoch: 49 train_error_ce: [0.28842927] train_error_mse: 0.006839449541109543 train_accuracy: 91.0 num_eg: 3000\n",
            "test_error_ce: [0.28842927] test_error_mse: 0.006839449541109515 test_accuracy: 91.0 num_eg: 3000\n",
            "epoch: 50 train_error_ce: [0.30888824] train_error_mse: 0.007185304414232863 train_accuracy: 90.5 num_eg: 3000\n",
            "test_error_ce: [0.30888824] test_error_mse: 0.007185304414232837 test_accuracy: 90.5 num_eg: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "667e24dc-af7e-4756-de5e-e25ea33ae9b5",
        "id": "ddeV9FfVAWS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "get_submission_csv(\"/content/drive/My Drive/testData.csv\",nn_1,\"submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000, 25)\n",
            "   id  label\n",
            "0   1      7\n",
            "1   2      9\n",
            "2   3      1\n",
            "3   4      4\n",
            "4   5      7\n",
            "5   6      4\n",
            "6   7      0\n",
            "7   8      1\n",
            "8   9      8\n",
            "9  10      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z54sF-yALVDy",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Neural Networks: 1 vs rest and boosting algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv35Ca6VLUgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_multiple(n_list,x,y,path=\"nan\",pred_on_xy=False):\n",
        "  if(pred_on_xy==True):  \n",
        "    data=np.array(list(zip(x,y)))\n",
        "    correct=0\n",
        "    for data_point,label in data :\n",
        "      out=[0]*10\n",
        "      data_point=data_point.reshape(len(data_point),1)\n",
        "      label=label.reshape(len(label),1)   \n",
        "      for n_i in n_list:\n",
        "        out[np.argmax(n_i.predict(data_point))]+=1\n",
        "      pred_out=np.argmax(out)\n",
        "      if(pred_out==np.argmax(label)):\n",
        "        correct+=1\n",
        "    accuracy=correct*100/len(x)\n",
        "    print(\"accuracy:\",accuracy)\n",
        "  if(path!=\"nan\"):\n",
        "    data=get_test_data(path)\n",
        "    predictions=[]\n",
        "    id=[]\n",
        "    i=1\n",
        "    for data_point in data:\n",
        "      out=[0]*10\n",
        "      data_point=data_point.reshape(len(data_point),1)\n",
        "      for n_i in n_list:\n",
        "        out[np.argmax(n_i.predict(datapoint))]+=1\n",
        "      predictions.append(np.argmax(out))\n",
        "      id.append(i)\n",
        "      i+=1  \n",
        "      submission=pd.DataFrame()\n",
        "    submission['id']=id\n",
        "    submission['label']=predictions\n",
        "    submission.to_csv(pred_csv_file_path,index=False)\n",
        "    print(submission.head(10))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf1IpNxbUIkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn=[nn_1,nn_2,nn_3,nn_4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt_Ko6IMT9tX",
        "colab_type": "code",
        "outputId": "10603311-ff6e-4a3d-a5ea-623ec2bd3333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_multiple(nn,val_x,val_y,\"nan\",True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 95.08\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}